from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python_operator import PythonOperator
from airflow.providers.amazon.aws.sensors.s3 import S3KeySensor
from airflow.providers.amazon.aws.operators.s3 import S3ListOperator
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook
dynamodb_hook = AwsBaseHook(aws_conn_id='aws_conn', region_name='us-east-1',client_type='dynamodb')
dynamodb_client = dynamodb_hook.get_client_type()

import shutil
import os
from datetime import datetime, timedelta
import tempfile
import pandas as pd
import boto3
import time

TEMPORARY_DYNAMODB_NAME="temporary-geolocation-dev-table"
S3_RAW_DATA_NAME="one-time-geolocation-local-test"
S3_STORAGE_DATA_NAME="storage-data-geolocation-glue-quality-test"

#Make a temporary directory for execution
TEMP_DIR = "/tmp/airflow_parquet_files"
if not os.path.exists(TEMP_DIR):
    os.makedirs(TEMP_DIR)

#Function to check if the entry was previously access checking in the dynamodb aux table
def dynamodb_check_previous_values(row):
    #Ensure date value in each row is string (prevents crashes)
    date_str = str(row['timestamp'])

    #Check if the datime value was previosly charge before
    response = dynamodb_client.get_item(TableName=TEMPORARY_DYNAMODB_NAME,Key={'unique_timestamp': {'S': date_str}})

    #If the value was not charged it's a fresh entry
    if 'Item' not in response:

        #Set TTL 4 days
        ttl_time = str(int(time.time()) + 4 * 24 * 60 * 60)

        #Add fresh datime to temporary auxiliar dynamodb table
        dynamodb_client.put_item(TableName=TEMPORARY_DYNAMODB_NAME,Item={'unique_timestamp': {'S': date_str}, 'TimeToExist': {'N': ttl_time}})

        return True
    #If not the value is a duplicated from previous days
    else:
        return False

#Function for download parquet task
def download_parquet_from_s3(**kwargs):
    #Get from xcom returned S3 Key
    ti = kwargs['ti']
    file_name = ti.xcom_pull(task_ids='list_s3_file')[0]

    #Hook connection
    hook = S3Hook('aws_conn')

    #Define local temporary path for object
    local_dir_path = f"{TEMP_DIR}/{file_name.split('.parquet',1)[0]}"
    local_file_path = os.path.join(local_dir_path, file_name)

    #Download S3 parquet object
    hook.download_file(key=file_name, bucket_name=S3_RAW_DATA_NAME, local_path=local_file_path, preserve_file_name=True, use_autogenerated_subdir=False)

    #Push to xcom local temporary path for object
    ti.xcom_push(key='local_file_path', value=local_file_path)

#Function for remove nulls task
def remove_nulls(**kwargs):
    #Get from xcom local temporary path for object
    ti = kwargs['ti']
    local_file_path = ti.xcom_pull(task_ids='download_parquet_from_s3', key='local_file_path')

    #Load data
    df_original = pd.read_parquet(local_file_path)

    ##########################################################################################################################Log print
    print(f"Raw data shape is {df_original.shape}")

    #Delete nulls
    df_no_null = df_original.dropna()

    #Overwrite local path
    df_no_null_path = local_file_path.replace('.parquet', '_no_null.parquet')

    #Convert to dataframe to parquet
    df_no_null.to_parquet(df_no_null_path)

    #Push to xcom local temporary path for object
    ti.xcom_push(key='no_null_file_path', value=df_no_null_path)

#Function for remove duplicated values task
def remove_duplicates(**kwargs):
    #Get from xcom local temporary path for object
    ti = kwargs['ti']
    no_null_file_path = ti.xcom_pull(task_ids='remove_nulls', key='no_null_file_path')

    #Load data
    df_no_null = pd.read_parquet(no_null_file_path)
    print(df_no_null.head())

    #Adjust data type
    df_no_null['timestamp'] = df_no_null['timestamp'].astype(str)

    #Delete duplicate values
    df_no_date_duplicate = df_no_null.drop_duplicates(subset=['timestamp'])

    #Overwrite local path
    df_no_date_duplicate_path = no_null_file_path.replace('_no_null.parquet', '_no_duplicates.parquet')

    #Convert to dataframe to parquet
    df_no_date_duplicate.to_parquet(df_no_date_duplicate_path)

    #Push to xcom local temporary path for object
    ti.xcom_push(key='no_duplicates_file_path', value=df_no_date_duplicate_path)

#Function to ckeck previous entries
def check_dynamodb_entries(**kwargs):
    #Get from xcom local temporary path for object
    ti = kwargs['ti']
    no_duplicates_file_path = ti.xcom_pull(task_ids='remove_duplicates', key='no_duplicates_file_path')

    #Load data
    df_no_date_duplicate = pd.read_parquet(no_duplicates_file_path)

    #Filter dataframe using custom defined function
    df_filtered = df_no_date_duplicate[df_no_date_duplicate.apply(dynamodb_check_previous_values, axis=1)]

    #Overwrite local path
    df_filtered_path = no_duplicates_file_path.replace('_no_duplicates.parquet', '_filtered.parquet')

    ##########################################################################################################Log print
    print(f"Loaded data shape is {df_filtered.shape}")
    
    #Change . to , and apply changes for parquet compatibility when saved
    df_filtered['latitude'] = df_filtered['latitude'].astype(float)
    df_filtered['longitude'] = df_filtered['longitude'].astype(float)
    df_filtered['timestamp'] = pd.to_datetime(df_filtered['timestamp'], format='%Y-%m-%dT%H:%M:%SZ', utc=True)

    #Convert to dataframe to parquet
    df_filtered.to_parquet(df_filtered_path)

    #Push to xcom local temporary path for object
    ti.xcom_push(key='filtered_file_path', value=df_filtered_path)

#Function to upload data to S3 destination task
def upload_parquet_to_s3(**kwargs):
    #Get from xcom local temporary path for object
    ti = kwargs['ti']
    filtered_file_path = ti.xcom_pull(task_ids='check_dynamodb_entries', key='filtered_file_path')

    #Get original filename
    file_name = ti.xcom_pull(task_ids='list_s3_file')[0]

    #Hook connection
    hook = S3Hook('aws_conn')

    #Load parquet file into destination bucket
    hook.load_file(filename=filtered_file_path, key=file_name, bucket_name=S3_STORAGE_DATA_NAME)


#Function to delete data from S3 source task
def delete_source_parquet_from_s3(**kwargs):
    #Get from xcom returned S3 Key
    ti = kwargs['ti']
    file_name = ti.xcom_pull(task_ids='list_s3_file')[0]

    #Hook connection
    hook = S3Hook('aws_conn')

    #Load parquet file into destination bucket
    hook.delete_objects(keys=file_name, bucket=S3_RAW_DATA_NAME)


#Function to ensure clean temporary directory
def clean_temp_dir(**kwargs):
    if os.path.exists(TEMP_DIR):
        shutil.rmtree(TEMP_DIR)

with DAG(
    dag_id='aws_etl_s3_to_S3_v5',
    description='aws_etl_s3_to_S3_v5',
    default_args = {
        'owner': 'andres',
        'depends_on_past': False,
        'start_date': datetime(2024, 7, 29, 19, 50),
        'email_on_failure': False,
        'email_on_retry': False,
        'retries': 5,
        'retry_delay': timedelta(minutes=2),
    },
    start_date=datetime(2024, 7, 29, 19, 50),
    schedule_interval='@daily',
    tags=['test'],
) as dag:

    #S3 Key Sensor task definition
    detect_new_file = S3KeySensor(
        task_id='sensor_s3',
        bucket_name=S3_RAW_DATA_NAME,
        bucket_key='*.parquet',
        wildcard_match=True,
        aws_conn_id='aws_conn',
        mode='poke',
        poke_interval=5,
        timeout=60
    )

    #S3 List task definition (return key name detected)
    list_files = S3ListOperator(
        task_id="list_s3_file",
        bucket=S3_RAW_DATA_NAME,
        prefix='input_records',
        delimiter='',
        aws_conn_id='aws_conn'
    )

    #Python task for download S3 Object
    download_parquet = PythonOperator(
        task_id='download_parquet_from_s3',
        python_callable=download_parquet_from_s3,
        provide_context=True,
    )

    #Python task remove nulls
    remove_nulls_task = PythonOperator(
        task_id='remove_nulls',
        python_callable=remove_nulls,
        provide_context=True,
    )

    #Python task remove duplicates
    remove_duplicates_task = PythonOperator(
        task_id='remove_duplicates',
        python_callable=remove_duplicates,
        provide_context=True,
    )

    #Python task filter records using dynamodb
    check_dynamodb_entries_task = PythonOperator(
        task_id='check_dynamodb_entries',
        python_callable=check_dynamodb_entries,
        provide_context=True,
    )

    #Python task to upload parquet into destination bucket
    upload_parquet_to_s3_task = PythonOperator(
        task_id='upload_parquet_to_s3',
        python_callable=upload_parquet_to_s3,
        provide_context=True,
    )

    #Python task delete data source bucket
    delete_source_parquet_from_s3_task = PythonOperator(
    task_id='delete_source_parquet_from_s3',
    python_callable=delete_source_parquet_from_s3,
    provide_context=True,
    )

    #Python task to clear temporary files
    clean_temp_files = PythonOperator(
    task_id='clean_temp_files',
    python_callable=clean_temp_dir,
    provide_context=True,
    )

    #Task workflow
    detect_new_file >> list_files >> download_parquet >> remove_nulls_task >> remove_duplicates_task >> check_dynamodb_entries_task >> upload_parquet_to_s3_task >> [clean_temp_files, delete_source_parquet_from_s3_task]
